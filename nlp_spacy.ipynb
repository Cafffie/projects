{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c79617c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from spacy.tokens import Span\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99582f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "i\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n"
     ]
    }
   ],
   "source": [
    "#Using blank pipeline\n",
    "\n",
    "nlp= spacy.blank(\"en\")\n",
    "doc= nlp(\"Captain america ate 100$ of samosa. Then he said i can do this all day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ae02a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b508d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pre-trained pipeline\n",
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93470ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5f08dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x273e333a750>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x273e3339c10>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x273e332dd90>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x273e366bfd0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x273e32258d0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x273e332db60>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca57fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  PROPN  |  Captain\n",
      "america  |  PROPN  |  america\n",
      "ate  |  VERB  |  eat\n",
      "100  |  NUM  |  100\n",
      "$  |  NUM  |  $\n",
      "of  |  ADP  |  of\n",
      "samosa  |  PROPN  |  samosa\n",
      ".  |  PUNCT  |  .\n",
      "Then  |  ADV  |  then\n",
      "he  |  PRON  |  he\n",
      "said  |  VERB  |  say\n",
      "i  |  PRON  |  I\n",
      "can  |  AUX  |  can\n",
      "do  |  VERB  |  do\n",
      "this  |  PRON  |  this\n",
      "all  |  DET  |  all\n",
      "day  |  NOUN  |  day\n",
      ".  |  PUNCT  |  .\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Captain america ate 100$ of samosa. Then he said i can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f41819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc\n",
      "$45 billion\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0538197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \",ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfa89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff5739e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7bac6",
   "metadata": {},
   "source": [
    "# Addin only NER to a blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e62187e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp= spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Add ner component from the english trained pipeline\n",
    "nlp.add_pipe(\"ner\", source= source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98a4b3",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41bd7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2869582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "adjustable  |  adjust\n",
      "rafting  |  raft\n",
      "ability  |  abil\n",
      "meeting  |  meet\n"
     ]
    }
   ],
   "source": [
    "words= [\"eating\", \"eats\", \"eat\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \" | \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f06b389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating eats eat adjustable rafting ability meeting'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words= [\"eating\", \"eats\", \"eat\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5938beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "doc= nlp(\"eating eats eat adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9525e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat  |  9837207709914848172\n",
      "eats  |  eat  |  9837207709914848172\n",
      "eat  |  eat  |  9837207709914848172\n",
      "adjustable  |  adjustable  |  6033511944150694480\n",
      "rafting  |  raft  |  7154368781129989833\n",
      "ability  |  ability  |  11565809527369121409\n",
      "meeting  |  meeting  |  14798207169164081740\n",
      "better  |  well  |  4525988469032889948\n"
     ]
    }
   ],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "doc= nlp(\"eating eats eat adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_, \" | \", token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ecc1e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mando  |  Mando  |  7837215228004622142\n",
      "talked  |  talk  |  13939146775466599234\n",
      "for  |  for  |  16037325823156266367\n",
      "3  |  3  |  602994839685422785\n",
      "hours  |  hour  |  9748623380567160636\n",
      "although  |  although  |  343236316598008647\n",
      "talking  |  talk  |  13939146775466599234\n",
      "is  |  be  |  10382539506755952630\n",
      "not  |  not  |  447765159362469301\n",
      "his  |  his  |  2661093235354845946\n",
      "thing  |  thing  |  2473243759842082748\n",
      "he  |  he  |  1655312771067108281\n",
      "became  |  become  |  12558846041070486771\n",
      "talkative  |  talkative  |  13364764166055324990\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Mando talked for 3 hours although talking is not his thing he became talkative\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_, \" | \", token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18018e2b",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9ef9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon  |  PROPN  |  96  |  proper noun\n",
      "flew  |  VERB  |  100  |  verb\n",
      "to  |  ADP  |  85  |  adposition\n",
      "mars  |  NOUN  |  92  |  noun\n",
      "yesterday  |  NOUN  |  92  |  noun\n",
      ".  |  PUNCT  |  97  |  punctuation\n",
      "He  |  PRON  |  95  |  pronoun\n",
      "carried  |  VERB  |  100  |  verb\n",
      "biryani  |  ADJ  |  84  |  adjective\n",
      "masala  |  NOUN  |  92  |  noun\n",
      "with  |  ADP  |  85  |  adposition\n",
      "him  |  PRON  |  95  |  pronoun\n",
      ".  |  PUNCT  |  97  |  punctuation\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Elon flew to mars yesterday. He carried biryani masala with him.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.pos, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "437822a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection\n",
      "!  |  PUNCT  |  punctuation\n",
      "Dr.  |  PROPN  |  proper noun\n",
      "Strange  |  PROPN  |  proper noun\n",
      "made  |  VERB  |  verb\n",
      "265  |  NUM  |  numeral\n",
      "million  |  NUM  |  numeral\n",
      "$  |  NUM  |  numeral\n",
      "on  |  ADP  |  adposition\n",
      "the  |  DET  |  determiner\n",
      "very  |  ADV  |  adverb\n",
      "first  |  ADJ  |  adjective\n",
      "day  |  NOUN  |  noun\n",
      ".  |  PUNCT  |  punctuation\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Wow! Dr. Strange made 265 million $ on the very first day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "968e6be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  UH  |  interjection\n",
      "!  |  PUNCT  |  .  |  punctuation mark, sentence closer\n",
      "Dr.  |  PROPN  |  NNP  |  noun, proper singular\n",
      "Strange  |  PROPN  |  NNP  |  noun, proper singular\n",
      "made  |  VERB  |  VBD  |  verb, past tense\n",
      "265  |  NUM  |  CD  |  cardinal number\n",
      "million  |  NUM  |  CD  |  cardinal number\n",
      "$  |  NUM  |  CD  |  cardinal number\n",
      "on  |  ADP  |  IN  |  conjunction, subordinating or preposition\n",
      "the  |  DET  |  DT  |  determiner\n",
      "very  |  ADV  |  RB  |  adverb\n",
      "first  |  ADJ  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "day  |  NOUN  |  NN  |  noun, singular or mass\n",
      ".  |  PUNCT  |  .  |  punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Wow! Dr. Strange made 265 million $ on the very first day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.tag_, \" | \" ,spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21424c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit  |  VBD  |  verb, past tense\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"He quit the job\")\n",
    "\n",
    "print(doc[1].text, \" | \", doc[1].tag_, \" | \", spacy.explain(doc[1].tag_) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "249eef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_text= \"\"\"Microsoft Corp. today announced the following results for the quarter ended September 30, 2023, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·        Revenue was $56.5 billion and increased 13% (up 12% in constant currency)\n",
    "\n",
    "·        Operating income was $26.9 billion and increased 25% (up 24% in constant currency)\n",
    "\n",
    "·        Net income was $22.3 billion and increased 27% (up 26% in constant currency)\n",
    "\n",
    "·        Diluted earnings per share was $2.99 and increased 27% (up 26% in constant currency)\n",
    "\n",
    "\"With copilots, we are making the age of AI real for people and businesses everywhere,\" said Satya Nadella, \n",
    "chairman and chief executive officer of Microsoft. \"We are rapidly infusing AI across every layer of the tech stack and for every role and \n",
    "business process to drive productivity gains for our customers\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9daca843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  |  PUNCT  |  ,  |  punctuation\n",
      ",  |  PUNCT  |  ,  |  punctuation\n",
      ":  |  PUNCT  |  :  |  punctuation\n",
      "\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n",
      "·  |  PUNCT  |  NFP  |  punctuation\n",
      "         |  SPACE  |  _SP  |  space\n",
      "(  |  PUNCT  |  -LRB-  |  punctuation\n",
      ")  |  PUNCT  |  -RRB-  |  punctuation\n",
      "\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n",
      "·  |  PUNCT  |  NFP  |  punctuation\n",
      "         |  SPACE  |  _SP  |  space\n",
      "(  |  PUNCT  |  -LRB-  |  punctuation\n",
      ")  |  PUNCT  |  -RRB-  |  punctuation\n",
      "\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n",
      "·  |  PUNCT  |  NFP  |  punctuation\n",
      "         |  SPACE  |  _SP  |  space\n",
      "(  |  PUNCT  |  -LRB-  |  punctuation\n",
      ")  |  PUNCT  |  -RRB-  |  punctuation\n",
      "\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n",
      "·  |  PUNCT  |  NFP  |  punctuation\n",
      "         |  SPACE  |  _SP  |  space\n",
      "(  |  PUNCT  |  -LRB-  |  punctuation\n",
      ")  |  PUNCT  |  -RRB-  |  punctuation\n",
      "\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n",
      "\"  |  PUNCT  |  ``  |  punctuation\n",
      ",  |  PUNCT  |  ,  |  punctuation\n",
      ",  |  PUNCT  |  ,  |  punctuation\n",
      "\"  |  PUNCT  |  ''  |  punctuation\n",
      ",  |  PUNCT  |  ,  |  punctuation\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n",
      ".  |  PUNCT  |  .  |  punctuation\n",
      "\"  |  PUNCT  |  ``  |  punctuation\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n",
      "\"  |  PUNCT  |  ''  |  punctuation\n",
      ".  |  PUNCT  |  .  |  punctuation\n",
      "\n",
      "  |  SPACE  |  _SP  |  space\n"
     ]
    }
   ],
   "source": [
    "#Extracting only SPACE\", \"X\", \"PUNCT\"\n",
    "doc= nlp(earnings_text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ in [\"SPACE\", \"X\", \"PUNCT\"]:\n",
    "        print(token, \" | \", token.pos_, \" | \", token.tag_, \" | \" ,spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0eec760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft  |  PROPN  |  NNP  |  proper noun\n",
      "Corp.  |  PROPN  |  NNP  |  proper noun\n",
      "today  |  NOUN  |  NN  |  noun\n",
      "announced  |  VERB  |  VBD  |  verb\n",
      "the  |  DET  |  DT  |  determiner\n",
      "following  |  VERB  |  VBG  |  verb\n",
      "results  |  NOUN  |  NNS  |  noun\n",
      "for  |  ADP  |  IN  |  adposition\n",
      "the  |  DET  |  DT  |  determiner\n",
      "quarter  |  NOUN  |  NN  |  noun\n",
      "ended  |  VERB  |  VBD  |  verb\n",
      "September  |  PROPN  |  NNP  |  proper noun\n",
      "30  |  NUM  |  CD  |  numeral\n",
      "2023  |  NUM  |  CD  |  numeral\n",
      "as  |  SCONJ  |  IN  |  subordinating conjunction\n",
      "compared  |  VERB  |  VBN  |  verb\n",
      "to  |  ADP  |  IN  |  adposition\n",
      "the  |  DET  |  DT  |  determiner\n",
      "corresponding  |  ADJ  |  JJ  |  adjective\n",
      "period  |  NOUN  |  NN  |  noun\n",
      "of  |  ADP  |  IN  |  adposition\n",
      "last  |  ADJ  |  JJ  |  adjective\n",
      "fiscal  |  ADJ  |  JJ  |  adjective\n",
      "year  |  NOUN  |  NN  |  noun\n",
      "Revenue  |  NOUN  |  NN  |  noun\n",
      "was  |  AUX  |  VBD  |  auxiliary\n",
      "$  |  SYM  |  $  |  symbol\n",
      "56.5  |  NUM  |  CD  |  numeral\n",
      "billion  |  NUM  |  CD  |  numeral\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "increased  |  VERB  |  VBD  |  verb\n",
      "13  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "up  |  ADV  |  RB  |  adverb\n",
      "12  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "in  |  ADP  |  IN  |  adposition\n",
      "constant  |  ADJ  |  JJ  |  adjective\n",
      "currency  |  NOUN  |  NN  |  noun\n",
      "Operating  |  VERB  |  VBG  |  verb\n",
      "income  |  NOUN  |  NN  |  noun\n",
      "was  |  AUX  |  VBD  |  auxiliary\n",
      "$  |  SYM  |  $  |  symbol\n",
      "26.9  |  NUM  |  CD  |  numeral\n",
      "billion  |  NUM  |  CD  |  numeral\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "increased  |  VERB  |  VBD  |  verb\n",
      "25  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "up  |  ADV  |  RB  |  adverb\n",
      "24  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "in  |  ADP  |  IN  |  adposition\n",
      "constant  |  ADJ  |  JJ  |  adjective\n",
      "currency  |  NOUN  |  NN  |  noun\n",
      "Net  |  ADJ  |  JJ  |  adjective\n",
      "income  |  NOUN  |  NN  |  noun\n",
      "was  |  AUX  |  VBD  |  auxiliary\n",
      "$  |  SYM  |  $  |  symbol\n",
      "22.3  |  NUM  |  CD  |  numeral\n",
      "billion  |  NUM  |  CD  |  numeral\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "increased  |  VERB  |  VBD  |  verb\n",
      "27  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "up  |  ADV  |  RB  |  adverb\n",
      "26  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "in  |  ADP  |  IN  |  adposition\n",
      "constant  |  ADJ  |  JJ  |  adjective\n",
      "currency  |  NOUN  |  NN  |  noun\n",
      "Diluted  |  VERB  |  VBN  |  verb\n",
      "earnings  |  NOUN  |  NNS  |  noun\n",
      "per  |  ADP  |  IN  |  adposition\n",
      "share  |  NOUN  |  NN  |  noun\n",
      "was  |  AUX  |  VBD  |  auxiliary\n",
      "$  |  SYM  |  $  |  symbol\n",
      "2.99  |  NUM  |  CD  |  numeral\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "increased  |  VERB  |  VBD  |  verb\n",
      "27  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "up  |  ADV  |  RB  |  adverb\n",
      "26  |  NUM  |  CD  |  numeral\n",
      "%  |  NOUN  |  NN  |  noun\n",
      "in  |  ADP  |  IN  |  adposition\n",
      "constant  |  ADJ  |  JJ  |  adjective\n",
      "currency  |  NOUN  |  NN  |  noun\n",
      "With  |  ADP  |  IN  |  adposition\n",
      "copilots  |  NOUN  |  NNS  |  noun\n",
      "we  |  PRON  |  PRP  |  pronoun\n",
      "are  |  AUX  |  VBP  |  auxiliary\n",
      "making  |  VERB  |  VBG  |  verb\n",
      "the  |  DET  |  DT  |  determiner\n",
      "age  |  NOUN  |  NN  |  noun\n",
      "of  |  ADP  |  IN  |  adposition\n",
      "AI  |  PROPN  |  NNP  |  proper noun\n",
      "real  |  ADJ  |  JJ  |  adjective\n",
      "for  |  ADP  |  IN  |  adposition\n",
      "people  |  NOUN  |  NNS  |  noun\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "businesses  |  NOUN  |  NNS  |  noun\n",
      "everywhere  |  ADV  |  RB  |  adverb\n",
      "said  |  VERB  |  VBD  |  verb\n",
      "Satya  |  PROPN  |  NNP  |  proper noun\n",
      "Nadella  |  PROPN  |  NNP  |  proper noun\n",
      "chairman  |  NOUN  |  NN  |  noun\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "chief  |  ADJ  |  JJ  |  adjective\n",
      "executive  |  ADJ  |  JJ  |  adjective\n",
      "officer  |  NOUN  |  NN  |  noun\n",
      "of  |  ADP  |  IN  |  adposition\n",
      "Microsoft  |  PROPN  |  NNP  |  proper noun\n",
      "We  |  PRON  |  PRP  |  pronoun\n",
      "are  |  AUX  |  VBP  |  auxiliary\n",
      "rapidly  |  ADV  |  RB  |  adverb\n",
      "infusing  |  VERB  |  VBG  |  verb\n",
      "AI  |  PROPN  |  NNP  |  proper noun\n",
      "across  |  ADP  |  IN  |  adposition\n",
      "every  |  DET  |  DT  |  determiner\n",
      "layer  |  NOUN  |  NN  |  noun\n",
      "of  |  ADP  |  IN  |  adposition\n",
      "the  |  DET  |  DT  |  determiner\n",
      "tech  |  NOUN  |  NN  |  noun\n",
      "stack  |  NOUN  |  NN  |  noun\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "for  |  ADP  |  IN  |  adposition\n",
      "every  |  DET  |  DT  |  determiner\n",
      "role  |  NOUN  |  NN  |  noun\n",
      "and  |  CCONJ  |  CC  |  coordinating conjunction\n",
      "business  |  NOUN  |  NN  |  noun\n",
      "process  |  NOUN  |  NN  |  noun\n",
      "to  |  PART  |  TO  |  particle\n",
      "drive  |  VERB  |  VB  |  verb\n",
      "productivity  |  NOUN  |  NN  |  noun\n",
      "gains  |  NOUN  |  NNS  |  noun\n",
      "for  |  ADP  |  IN  |  adposition\n",
      "our  |  PRON  |  PRP$  |  pronoun\n",
      "customers  |  NOUN  |  NNS  |  noun\n"
     ]
    }
   ],
   "source": [
    "#Removing punctuations and irrelevance\n",
    "doc= nlp(earnings_text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"X\", \"PUNCT\"]:\n",
    "        print(token, \" | \", token.pos_, \" | \", token.tag_, \" | \" ,spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f25af8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(earnings_text)\n",
    "filtered_text= []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"X\", \"PUNCT\"]:\n",
    "        filtered_text.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0604b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Microsoft,\n",
       " Corp.,\n",
       " today,\n",
       " announced,\n",
       " the,\n",
       " following,\n",
       " results,\n",
       " for,\n",
       " the,\n",
       " quarter]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82b0a448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 8,\n",
       " 92: 37,\n",
       " 100: 14,\n",
       " 90: 7,\n",
       " 85: 16,\n",
       " 93: 17,\n",
       " 97: 24,\n",
       " 98: 1,\n",
       " 84: 11,\n",
       " 103: 12,\n",
       " 87: 6,\n",
       " 99: 4,\n",
       " 89: 8,\n",
       " 86: 6,\n",
       " 95: 3,\n",
       " 94: 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count= doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68c43c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[96].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a02cc64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN  |  8\n",
      "NOUN  |  37\n",
      "VERB  |  14\n",
      "DET  |  7\n",
      "ADP  |  16\n",
      "NUM  |  17\n",
      "PUNCT  |  24\n",
      "SCONJ  |  1\n",
      "ADJ  |  11\n",
      "SPACE  |  12\n",
      "AUX  |  6\n",
      "SYM  |  4\n",
      "CCONJ  |  8\n",
      "ADV  |  6\n",
      "PRON  |  3\n",
      "PART  |  1\n"
     ]
    }
   ],
   "source": [
    "for k, v in count.items():\n",
    "    print(doc.vocab[k].text, \" | \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b067ed6",
   "metadata": {},
   "source": [
    "# Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a2f934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "doc= nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ad12a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cff13d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels[\"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e042951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micheal Bloomberg  |  PERSON  |  People, including fictional\n",
      "Bloomberg Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "1982  |  DATE  |  Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Micheal Bloomberg founded Bloomberg Inc in 1982\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,  \" | \", ent.label_,  \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce1c08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging face name entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8fb8b",
   "metadata": {},
   "source": [
    "# SPAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9fcec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53010ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "s1= Span(doc, 0, 1, label=\"ORG\")\n",
    "s2= Span(doc, 6, 7, label=\"ORG\")\n",
    "\n",
    "doc.set_ents([s1, s2], default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "036058ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG\n",
      "twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,  \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857263ea",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1a235ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 5, 'hathodawala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v= CountVectorizer()\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b310e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 12,\n",
       " 'hathodawala': 2,\n",
       " 'is': 5,\n",
       " 'looking': 9,\n",
       " 'for': 0,\n",
       " 'job': 8,\n",
       " 'thor hathodawala': 13,\n",
       " 'hathodawala is': 3,\n",
       " 'is looking': 6,\n",
       " 'looking for': 10,\n",
       " 'for job': 1,\n",
       " 'thor hathodawala is': 14,\n",
       " 'hathodawala is looking': 4,\n",
       " 'is looking for': 7,\n",
       " 'looking for job': 11}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v= CountVectorizer(ngram_range=(1,3))\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7ac6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    \"Wunmi ate rice and beans.\",\n",
    "    \"Funke is beautiful and smart.\",\n",
    "    \"Nigerians are friendly and loves eating jollof rice.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f132149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stopwords, punctuation and lemmatize\n",
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    doc= nlp(text)\n",
    "    filtered_tokens= []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "606fd35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nigerians friendly love eat jollof rice'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying the function to a text\n",
    "preprocess(\"Nigerians are friendly and loves eating jollof rice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38505d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wunmi eat rice bean',\n",
       " 'Funke beautiful smart',\n",
       " 'Nigerians friendly love eat jollof rice']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus= [preprocess(text) for text in corpus]\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c15d3",
   "metadata": {},
   "source": [
    "# BAG OF NGRAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e80a787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wunmi': 19,\n",
       " 'eat': 3,\n",
       " 'rice': 16,\n",
       " 'bean': 0,\n",
       " 'wunmi eat': 20,\n",
       " 'eat rice': 5,\n",
       " 'rice bean': 17,\n",
       " 'funke': 8,\n",
       " 'beautiful': 1,\n",
       " 'smart': 18,\n",
       " 'funke beautiful': 9,\n",
       " 'beautiful smart': 2,\n",
       " 'nigerians': 14,\n",
       " 'friendly': 6,\n",
       " 'love': 12,\n",
       " 'jollof': 10,\n",
       " 'nigerians friendly': 15,\n",
       " 'friendly love': 7,\n",
       " 'love eat': 13,\n",
       " 'eat jollof': 4,\n",
       " 'jollof rice': 11}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v= CountVectorizer(ngram_range=(1,2))\n",
    "v.fit(processed_corpus)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a958f48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bean', 'beautiful', 'beautiful smart', 'eat', 'eat jollof',\n",
       "       'eat rice', 'friendly', 'friendly love', 'funke',\n",
       "       'funke beautiful', 'jollof', 'jollof rice', 'love', 'love eat',\n",
       "       'nigerians', 'nigerians friendly', 'rice', 'rice bean', 'smart',\n",
       "       'wunmi', 'wunmi eat'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "607e917d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Wunmi ate rice and beans.\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6bd2bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform(processed_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07310b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wunmi | Wunmi | 17622258296457287139\n",
      "ate | eat | 9837207709914848172\n",
      "rice | rice | 5999186075793416517\n",
      "and | and | 2283656566040971221\n",
      "beans | bean | 4990996348219001211\n",
      ". | . | 12646065887601541794\n",
      "Funke | Funke | 14706415696644233707\n",
      "is | be | 10382539506755952630\n",
      "beautiful | beautiful | 530855179026533974\n",
      "and | and | 2283656566040971221\n",
      "smart | smart | 2191600787884973499\n",
      ". | . | 12646065887601541794\n",
      "          |           | 7263008856362461915\n",
      "Nigerians | Nigerians | 2558192113150403045\n",
      "are | be | 10382539506755952630\n",
      "friendly | friendly | 12034322203066787430\n",
      "and | and | 2283656566040971221\n",
      "loves | love | 3702023516439754181\n",
      "eatting | eatte | 17271744586191902617\n",
      "jollof | jollof | 2559203094161453931\n",
      "rice | rice | 5999186075793416517\n",
      ". | . | 12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"Wunmi ate rice and beans. Funke is beautiful and smart. \\\n",
    "         Nigerians are friendly and loves eatting jollof rice.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_, \"|\", token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aaf0cb",
   "metadata": {},
   "source": [
    "# TFIDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b685d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= [\"\"\"Incredible Gifts India Wooden Happy Birthday Unique Personalized Gift (5 X 4 Inch) Size:4 x 5   \n",
    "Made Of Natural Imported Wood, Which Is Quite Solid With Light Particle Pattern & Is Soft Pale To Blond Colour. \n",
    "Your Uploaded Photo Will Look Amazing And Beautiful After Laser Engraving On It. This Is One Of The Most Popular Unique Gifts In Our Store. \n",
    "We Offer This In Multiple Sizes, \n",
    "Some Can Be Used As Table Top And The Big Sizes Can Be Used As Wall Hanging Which Just Blends With Your Home Decaration. \n",
    "You Just Need To Upload A Picture And Add Your Own Text And We Will Do The Rest For You. \n",
    "We Will Email You The Preview Before Making The Final Product. \n",
    "Do You Want The Best Moment Of Your Life To Be Engraved On A Wooden Plaque That Lasts For A Longer Time And Stays Close To You Forever? \n",
    "Then You Are At The Right Place. \n",
    "We Present To You Various Sizes Personalized Engraved Wooden Plaques Made With Birch Wood. \n",
    "Let Your Memories Be Engraved On Wooden Plaques And Stay With Your Forever.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1a2c991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x109 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 109 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv= TfidfVectorizer()\n",
    "cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c412365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['add', 'after', 'amazing', 'and', 'are', 'as', 'at', 'be',\n",
       "       'beautiful', 'before', 'best', 'big', 'birch', 'birthday',\n",
       "       'blends', 'blond', 'can', 'close', 'colour', 'decaration', 'do',\n",
       "       'email', 'engraved', 'engraving', 'final', 'for', 'forever',\n",
       "       'gift', 'gifts', 'hanging', 'happy', 'home', 'imported', 'in',\n",
       "       'inch', 'incredible', 'india', 'is', 'it', 'just', 'laser',\n",
       "       'lasts', 'let', 'life', 'light', 'longer', 'look', 'made',\n",
       "       'making', 'memories', 'moment', 'most', 'multiple', 'natural',\n",
       "       'need', 'of', 'offer', 'on', 'one', 'our', 'own', 'pale',\n",
       "       'particle', 'pattern', 'personalized', 'photo', 'picture', 'place',\n",
       "       'plaque', 'plaques', 'popular', 'present', 'preview', 'product',\n",
       "       'quite', 'rest', 'right', 'size', 'sizes', 'soft', 'solid', 'some',\n",
       "       'stay', 'stays', 'store', 'table', 'text', 'that', 'the', 'then',\n",
       "       'this', 'time', 'to', 'top', 'unique', 'upload', 'uploaded',\n",
       "       'used', 'various', 'wall', 'want', 'we', 'which', 'will', 'with',\n",
       "       'wood', 'wooden', 'you', 'your'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3be81755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'incredible': 35, 'gifts': 28, 'india': 36, 'wooden': 106, 'happy': 30, 'birthday': 13, 'unique': 94, 'personalized': 64, 'gift': 27, 'inch': 34, 'size': 77, 'made': 47, 'of': 55, 'natural': 53, 'imported': 32, 'wood': 105, 'which': 102, 'is': 37, 'quite': 74, 'solid': 80, 'with': 104, 'light': 44, 'particle': 62, 'pattern': 63, 'soft': 79, 'pale': 61, 'to': 92, 'blond': 15, 'colour': 18, 'your': 108, 'uploaded': 96, 'photo': 65, 'will': 103, 'look': 46, 'amazing': 2, 'and': 3, 'beautiful': 8, 'after': 1, 'laser': 40, 'engraving': 23, 'on': 57, 'it': 38, 'this': 90, 'one': 58, 'the': 88, 'most': 51, 'popular': 70, 'in': 33, 'our': 59, 'store': 84, 'we': 101, 'offer': 56, 'multiple': 52, 'sizes': 78, 'some': 81, 'can': 16, 'be': 7, 'used': 97, 'as': 5, 'table': 85, 'top': 93, 'big': 11, 'wall': 99, 'hanging': 29, 'just': 39, 'blends': 14, 'home': 31, 'decaration': 19, 'you': 107, 'need': 54, 'upload': 95, 'picture': 66, 'add': 0, 'own': 60, 'text': 86, 'do': 20, 'rest': 75, 'for': 25, 'email': 21, 'preview': 72, 'before': 9, 'making': 48, 'final': 24, 'product': 73, 'want': 100, 'best': 10, 'moment': 50, 'life': 43, 'engraved': 22, 'plaque': 68, 'that': 87, 'lasts': 41, 'longer': 45, 'time': 91, 'stays': 83, 'close': 17, 'forever': 26, 'then': 89, 'are': 4, 'at': 6, 'right': 76, 'place': 67, 'present': 71, 'various': 98, 'plaques': 69, 'birch': 12, 'let': 42, 'memories': 49, 'stay': 82}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "482a6843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'incredible'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afb31121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['incredible', 'india', 'is', 'it', 'just', 'laser', 'lasts', 'let',\n",
       "       'life', 'light', 'longer', 'look', 'made', 'making', 'memories'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()[35:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "404abf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add : 1.0\n",
      "after : 1.0\n",
      "amazing : 1.0\n",
      "and : 1.0\n",
      "are : 1.0\n",
      "as : 1.0\n",
      "at : 1.0\n",
      "be : 1.0\n",
      "beautiful : 1.0\n",
      "before : 1.0\n",
      "best : 1.0\n",
      "big : 1.0\n",
      "birch : 1.0\n",
      "birthday : 1.0\n",
      "blends : 1.0\n",
      "blond : 1.0\n",
      "can : 1.0\n",
      "close : 1.0\n",
      "colour : 1.0\n",
      "decaration : 1.0\n",
      "do : 1.0\n",
      "email : 1.0\n",
      "engraved : 1.0\n",
      "engraving : 1.0\n",
      "final : 1.0\n",
      "for : 1.0\n",
      "forever : 1.0\n",
      "gift : 1.0\n",
      "gifts : 1.0\n",
      "hanging : 1.0\n",
      "happy : 1.0\n",
      "home : 1.0\n",
      "imported : 1.0\n",
      "in : 1.0\n",
      "inch : 1.0\n",
      "incredible : 1.0\n",
      "india : 1.0\n",
      "is : 1.0\n",
      "it : 1.0\n",
      "just : 1.0\n",
      "laser : 1.0\n",
      "lasts : 1.0\n",
      "let : 1.0\n",
      "life : 1.0\n",
      "light : 1.0\n",
      "longer : 1.0\n",
      "look : 1.0\n",
      "made : 1.0\n",
      "making : 1.0\n",
      "memories : 1.0\n",
      "moment : 1.0\n",
      "most : 1.0\n",
      "multiple : 1.0\n",
      "natural : 1.0\n",
      "need : 1.0\n",
      "of : 1.0\n",
      "offer : 1.0\n",
      "on : 1.0\n",
      "one : 1.0\n",
      "our : 1.0\n",
      "own : 1.0\n",
      "pale : 1.0\n",
      "particle : 1.0\n",
      "pattern : 1.0\n",
      "personalized : 1.0\n",
      "photo : 1.0\n",
      "picture : 1.0\n",
      "place : 1.0\n",
      "plaque : 1.0\n",
      "plaques : 1.0\n",
      "popular : 1.0\n",
      "present : 1.0\n",
      "preview : 1.0\n",
      "product : 1.0\n",
      "quite : 1.0\n",
      "rest : 1.0\n",
      "right : 1.0\n",
      "size : 1.0\n",
      "sizes : 1.0\n",
      "soft : 1.0\n",
      "solid : 1.0\n",
      "some : 1.0\n",
      "stay : 1.0\n",
      "stays : 1.0\n",
      "store : 1.0\n",
      "table : 1.0\n",
      "text : 1.0\n",
      "that : 1.0\n",
      "the : 1.0\n",
      "then : 1.0\n",
      "this : 1.0\n",
      "time : 1.0\n",
      "to : 1.0\n",
      "top : 1.0\n",
      "unique : 1.0\n",
      "upload : 1.0\n",
      "uploaded : 1.0\n",
      "used : 1.0\n",
      "various : 1.0\n",
      "wall : 1.0\n",
      "want : 1.0\n",
      "we : 1.0\n",
      "which : 1.0\n",
      "will : 1.0\n",
      "with : 1.0\n",
      "wood : 1.0\n",
      "wooden : 1.0\n",
      "you : 1.0\n",
      "your : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Geting the idf score of the words\n",
    "feature_names= cv.get_feature_names_out()\n",
    "\n",
    "for word in feature_names:\n",
    "    feature_index= cv.vocabulary_.get(word)\n",
    "    print(word, \":\", cv.idf_[feature_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed0c84",
   "metadata": {},
   "source": [
    "# SPACY WORD VECTOR (WORD EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a09775e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a297c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(\"\"\"Incredible Gifts India Wooden Happy Birthday Unique Personalized Gift (5 X 4 Inch) Size:4 x 5   \n",
    "Made Of Natural Imported Wood, Which Is Quite Solid With Light Particle Pattern & Is Soft Pale To Blond Colour. \n",
    "Your Uploaded Photo Will Look Amazing And Beautiful After Laser Engraving On It. This Is One Of The Most Popular Unique Gifts In Our Store. \n",
    "We Offer This In Multiple Sizes, \n",
    "Some Can Be Used As Table Top And The Big Sizes Can Be Used As Wall Hanging Which Just Blends With Your Home Decaration. \n",
    "You Just Need To Upload A Picture And Add Your Own Text And We Will Do The Rest For You. \n",
    "We Will Email You The Preview Before Making The Final Product. \n",
    "Do You Want The Best Moment Of Your Life To Be Engraved On A Wooden Plaque That Lasts For A Longer Time And Stays Close To You Forever? \n",
    "Then You Are At The Right Place. \n",
    "We Present To You Various Sizes Personalized Engraved Wooden Plaques Made With Birch Wood. \n",
    "Let Your Memories Be Engraved On Wooden Plaques And Stay With Your Forever.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0020e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49471596, -0.44727975, -1.3359691 , -0.3698964 ,  2.3133612 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a4da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
